{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f5a5aa-e485-49fd-80b1-48852d0db23a",
   "metadata": {},
   "source": [
    "# Reward Model testing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141c449",
   "metadata": {},
   "source": [
    "In [Chai Prize Reward Model ‚Äî Part I: Data](https://wild-chatter-b52.notion.site/Chai-Prize-Reward-Model-Part-I-Data-026a10a8998a404ca6a52251c0c8d052) and [Chai Prize Reward Model ‚Äî Part II: Training Model](https://wild-chatter-b52.notion.site/Chai-Prize-Reward-Model-Part-II-Training-model-753b574c843f4d0780bf8d85b084da57), we introduced the basics of reward model, how to make a dataset, and how to train one.  \n",
    "Now all the ingredients are ready! We prepared this notebook for you to explore the convenience of fitting your own reward model with Chaiverse reward model training pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5d5a2-0b6c-4de7-8653-9fd208b44649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import huggingface_hub\n",
    "\n",
    "import chai_guanaco as chai\n",
    "\n",
    "from chaiverse.dataset import DatasetLoader, RewardDatasetBuilder\n",
    "from chaiverse.tokenizer import GPT2Tokenizer\n",
    "from chaiverse.model.reward_model import RewardClassificationTrainer, RewardRegressionTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72d1e4-c148-4e36-87be-8dc28d02226d",
   "metadata": {},
   "source": [
    "## login setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a1b056-a1ca-4f5a-b7cf-137137ca58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chai.developer_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806da82e-eb85-47d4-9814-7629c2184ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835bb2c-7fe0-44f1-8ccc-0be26a2106b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037077ee-0506-431d-9a58-3cb8e02e1262",
   "metadata": {},
   "source": [
    "## Load and process a small dataset\n",
    "\n",
    "Download [chai prize reward model dataset](https://huggingface.co/datasets/ChaiML/20231012_chai_prize_reward_model_data) from Huggingface, we shuffle dataset then split 1% dataset as validation fold. The DatasetDict looks as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d69d31-a724-41ae-87f0-bfc704499820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = 'ChaiML/20231012_chai_prize_reward_model_data'\n",
    "data_loader = DatasetLoader(\n",
    "        hf_path=data_path,\n",
    "        data_samples=1000,\n",
    "        validation_split_size=0.1,\n",
    "        shuffle=True,\n",
    "        )\n",
    "df = data_loader.load()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample of the data. It includes `input_text` - the conversation, and `labels` - wether or not user gave it a thumbs up.\n",
    "# The labels here are 1 or 0, perfect for classification job. \n",
    "df['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbca1e",
   "metadata": {},
   "source": [
    "### Base model / tokenizer\n",
    "\n",
    "In this feedback dataset, we use thumbs up as single-label targets, which means we can directly train with sequence classification/regression method. Here, we use `gpt2` as base model. It would be easier for us to compare performance with Chai's in-house gpt2 reward model.\n",
    "\n",
    "- `padding_side` of the tokenizer should match the base model‚Äôs config.\n",
    "- `truncation_side = 'left'`  ensures that the most recent responses are included in input.\n",
    "- `block_size = 512` which can be expanded to 1024 as gpt2 max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec442b84-a0d8-4afd-a422-a0d31f21b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "tokenizer_loader = GPT2Tokenizer(\n",
    "        padding_side='right',\n",
    "        truncation_side='left',\n",
    "        )\n",
    "data_builder = RewardDatasetBuilder(\n",
    "        tokenizer_loader=tokenizer_loader,\n",
    "        block_size=512,\n",
    "        )\n",
    "data = data_builder.generate(df)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeee02b-19ec-47f7-8b67-bd8429288cf2",
   "metadata": {},
   "source": [
    "## model setup and fitting\n",
    "\n",
    "- If using `RewardRegressionTrainer`, we treat it as a regression task, default `num_labels` equal to 1.\n",
    "- If using `RewardClassificationTrainer`, we treat it as a single-label classification task, default `num_labels` equal to 2.\n",
    "- It‚Äôs important to assign the same tokenizer_loader to ensure the training step and inference step have the same processing configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb72042-a60b-46d1-bcc4-6c4e90276e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train setup\n",
    "# set bf16=False if you are not using gpu\n",
    "model = RewardClassificationTrainer(\n",
    "        model_name='gpt2',\n",
    "        tokenizer_loader=tokenizer_loader,\n",
    "        output_dir='test_reward_model',\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=1,\n",
    "        bf16=True,\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=2,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=2\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111a68f",
   "metadata": {},
   "source": [
    "We can also build RewawrdRegressionTrainer, with similar code. \n",
    "\n",
    "```\n",
    "model = RewardClassificationTrainer(\n",
    "        model_name='gpt2',\n",
    "        tokenizer_loader=tokenizer_loader,\n",
    "        output_dir='test_reward_model',\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=1,\n",
    "        bf16=True,\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=2,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=2\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9588d63-d56b-43f5-9751-6575fe0fba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model initialisation and fitting\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535fce1-9f59-4026-ab37-c49884425640",
   "metadata": {},
   "source": [
    "## upload the reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40c8a0-c2e5-4b18-aec2-cf23d0df5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The destination model path on your huggingface\n",
    "reward_url = ''\n",
    "\n",
    "#push model to huggingface\n",
    "model.push_to_hub(reward_url,private=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8deae",
   "metadata": {},
   "source": [
    "## Submit to competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The base model url\n",
    "model_url = \"\"\n",
    "\n",
    "#To upload reward model, need to add \"reward_repo\" in submission_parameters\n",
    "submission_parameters = {\n",
    "\t\"model_repo\": model_url,\n",
    "  \"reward_repo\": reward_url,\n",
    "\t\"generation_params\": {\n",
    "\t\t\"temperature\": 0.99,\n",
    "    \"top_p\": 0.2,\n",
    "    \"top_k\": 40,\n",
    "    \"stopping_words\": ['\\n'],\n",
    "    \"presence_penalty\": 0.,\n",
    "    \"frequency_penalty\": 0.,\n",
    "    \"max_input_tokens\": 2048\n",
    "\t},\n",
    "  'model_name': 'My-First-RwardModel',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff83069",
   "metadata": {},
   "outputs": [],
   "source": [
    "submitter = chai.ModelSubmitter()\n",
    "submission_id = submitter.submit(submission_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1dd0dc",
   "metadata": {},
   "source": [
    "# Yay! You just trained and submitted a reward model! \n",
    "\n",
    "Make sure you check out the writeup [Chai Prize Reward Model ‚Äî Part II: Training model](https://wild-chatter-b52.notion.site/Chai-Prize-Reward-Model-Part-II-Training-model-753b574c843f4d0780bf8d85b084da57) to see how the reward model massively improved the performance of the base model, and ranked to the top of the competition!\n",
    "\n",
    "Reward model + best_of_N sampling is an effective way to improve model performance. It looks simple and straightforward, but you do need a lot of detailed attention to make it work. And obviously, there is a lot of space to explore for reward model improvement.\n",
    "\n",
    "Feel free to play with this notebook and test out different methods to make a better model! \n",
    "Maybe something like...\n",
    "- Better data filtering/cleaning/formatting üß†\n",
    "- Different models: gpt2, deberta, roberta, phi-1.5 üèãÔ∏è\n",
    "- Parameter optimization üõ†Ô∏è\n",
    "- New targets from feedback dataset or external data üéØ\n",
    "- Multi-labels objects, weighted loss, joint reward tasks ü§π\n",
    "- Pair-wise reward model ‚öñÔ∏è\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
